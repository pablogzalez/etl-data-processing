services:
  # Servicios existentes (postgres, airflow, etc.)

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.17.9
    container_name: elasticsearch
    environment:
      - ES_JAVA_OPTS=${ES_JAVA_OPTS}
      - discovery.type=${DISCOVERY_TYPE}
      - xpack.security.enabled=${XPACK_SECURITY_ENABLED}
    ports:
      - "${ELASTICSEARCH_PORT}:9200"
    volumes:
      - esdata:/usr/share/elasticsearch/data

  kibana:
    image: docker.elastic.co/kibana/kibana:7.17.9
    container_name: kibana
    ports:
      - "${KIBANA_PORT}:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:${ELASTICSEARCH_PORT}
    depends_on:
      - elasticsearch

  postgres:
    image: postgres:13
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
    volumes:
      - postgres_db_volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER}"]
      interval: 10s
      retries: 5

  airflow-init:
    build:
      context: ../
      dockerfile: docker/Dockerfile
    environment:
      - AIRFLOW__CORE__EXECUTOR=${AIRFLOW__CORE__EXECUTOR}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW__CORE__FERNET_KEY}
      - AIRFLOW__WEBSERVER__SECRET_KEY=${AIRFLOW__WEBSERVER__SECRET_KEY}
      - AIRFLOW_USERNAME=${AIRFLOW_USERNAME}
      - AIRFLOW_FIRSTNAME=${AIRFLOW_FIRSTNAME}
      - AIRFLOW_LASTNAME=${AIRFLOW_LASTNAME}
      - AIRFLOW_EMAIL=${AIRFLOW_EMAIL}
      - AIRFLOW_PASSWORD=${AIRFLOW_PASSWORD}
    depends_on:
      postgres:
        condition: service_healthy
    entrypoint: /bin/bash -c "airflow db init && airflow users create --username ${AIRFLOW_USERNAME} --firstname ${AIRFLOW_FIRSTNAME} --lastname ${AIRFLOW_LASTNAME} --role Admin --email ${AIRFLOW_EMAIL} --password ${AIRFLOW_PASSWORD}"
    volumes:
      - ../dags:/opt/airflow/dags
      - ../data/input:/opt/airflow/data/input
      - ../data/output:/opt/airflow/data/output
      - ../scripts:/opt/airflow/scripts

  airflow-webserver:
    build:
      context: ../
      dockerfile: docker/Dockerfile
    restart: always
    environment:
      - PYTHONPATH=${PYTHONPATH}
      - AIRFLOW__WEBSERVER__WORKERS=${AIRFLOW__WEBSERVER__WORKERS}
      - AIRFLOW__CORE__EXECUTOR=${AIRFLOW__CORE__EXECUTOR}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW__CORE__FERNET_KEY}
      - AIRFLOW__WEBSERVER__SECRET_KEY=${AIRFLOW__WEBSERVER__SECRET_KEY}
    depends_on:
      - airflow-init
    ports:
      - "${AIRFLOW_WEB_PORT}:8080"
    command: webserver
    volumes:
      - ../dags:/opt/airflow/dags
      - ../data/input:/opt/airflow/data/input
      - ../data/output:/opt/airflow/data/output
      - ../scripts:/opt/airflow/scripts

  airflow-scheduler:
    build:
      context: ../
      dockerfile: docker/Dockerfile
    restart: always
    environment:
      - PYTHONPATH=${PYTHONPATH}
      - AIRFLOW__CORE__EXECUTOR=${AIRFLOW__CORE__EXECUTOR}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW__CORE__FERNET_KEY}
      - AIRFLOW__WEBSERVER__SECRET_KEY=${AIRFLOW__WEBSERVER__SECRET_KEY}
    depends_on:
      airflow-init:
        condition: service_started
    entrypoint: /bin/bash -c "sleep 10; airflow scheduler"
    volumes:
      - ../dags:/opt/airflow/dags
      - ../data/input:/opt/airflow/data/input
      - ../data/output:/opt/airflow/data/output
      - ../scripts:/opt/airflow/scripts

volumes:
  postgres_db_volume:
  esdata:
